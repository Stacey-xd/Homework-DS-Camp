{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load raw text files\n",
    "with open(\"data/rt-polarity.neg\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    texts_neg = f.read().splitlines()\n",
    "with open(\"data/rt-polarity.pos\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    texts_pos = f.read().splitlines()\n",
    "\n",
    "# Combine texts and labels into a DataFrame\n",
    "data = [(text, 0) for text in texts_neg] + [(text, 1) for text in texts_pos]\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
    "\n",
    "# Shuffle and split into train/test sets\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Independent Evaluation Data and Helper Function\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Independent sentences and their true labels\n",
    "independent_eval_data = {\n",
    "    \"sentences\": [\n",
    "        \"I can't believe I wasted two hours on this.\",\n",
    "        \"Absolutely stunning visuals and sound.\",\n",
    "        \"Nothing made sense from start to finish.\",\n",
    "        \"I would gladly watch it again.\",\n",
    "        \"The characters were flat and uninteresting.\",\n",
    "        \"A truly heartwarming story.\",\n",
    "        \"Regretted not walking out halfway.\",\n",
    "        \"The pacing was perfect throughout.\",\n",
    "        \"It felt like a school project.\",\n",
    "        \"One of the best experiences I've had recently.\",\n",
    "        \"I didn't expect much, and I was right.\",\n",
    "        \"Everything came together beautifully.\",\n",
    "        \"It lacked soul and depth.\",\n",
    "        \"A breath of fresh air!\",\n",
    "        \"I've seen better student films.\",\n",
    "        \"Brilliantly acted and directed.\",\n",
    "        \"It was just okay, nothing more.\",\n",
    "        \"Truly inspiring and thoughtful.\",\n",
    "        \"Worst decision to spend money on this.\",\n",
    "        \"Delivers on every level.\",\n",
    "        \"I nearly fell asleep watching it.\",\n",
    "        \"Left me with tears of joy.\",\n",
    "        \"Painfully dull and repetitive.\",\n",
    "        \"I felt connected to every character.\",\n",
    "        \"Should've trusted the negative reviews.\",\n",
    "        \"Exceeded all expectations!\",\n",
    "        \"The dialogue felt forced and fake.\",\n",
    "        \"I'll remember this for years to come.\",\n",
    "        \"Just another generic story.\",\n",
    "        \"Packed with emotion and honesty.\",\n",
    "        \"Everything was overacted.\",\n",
    "        \"An instant favorite of mine.\",\n",
    "        \"Confusing and messy throughout.\",\n",
    "        \"The cinematography was breathtaking.\",\n",
    "        \"Avoid at all costs.\",\n",
    "        \"Finally, something original!\",\n",
    "        \"I couldn’t finish it.\",\n",
    "        \"Perfect from start to finish.\",\n",
    "        \"So bad it’s almost good.\",\n",
    "        \"Uplifting and magical.\",\n",
    "        \"The twist was completely predictable.\",\n",
    "        \"Left me speechless in the best way.\",\n",
    "        \"Cliché after cliché.\",\n",
    "        \"Creative and full of surprises.\",\n",
    "        \"What a waste of potential.\",\n",
    "        \"Full of charm and wit.\",\n",
    "        \"Annoyingly loud soundtrack.\",\n",
    "        \"Simply delightful!\",\n",
    "        \"The plot went nowhere.\",\n",
    "        \"A total joy to watch.\",\n",
    "        \"I’ve never cringed so much.\",\n",
    "        \"Soothing and beautiful tone.\",\n",
    "        \"Disappointing in every way.\",\n",
    "        \"Deserves an award.\",\n",
    "        \"Awkward performances throughout.\",\n",
    "        \"A rare gem these days.\",\n",
    "        \"I expected more from this director.\",\n",
    "        \"Left me smiling the whole time.\",\n",
    "        \"Boring, dry, and uneventful.\",\n",
    "        \"Smart, funny, and touching.\",\n",
    "        \"Felt like a chore to sit through.\",\n",
    "        \"Absolutely nailed it.\",\n",
    "        \"Zero chemistry between the leads.\",\n",
    "        \"Full of heart and meaning.\",\n",
    "        \"Not even my dog enjoyed it.\",\n",
    "        \"I didn’t want it to end.\",\n",
    "        \"Cringe-worthy in all the wrong ways.\",\n",
    "        \"Memorable and masterfully told.\",\n",
    "        \"Felt rushed and incomplete.\",\n",
    "        \"A celebration of storytelling.\",\n",
    "        \"I'd rather do taxes than watch this again.\",\n",
    "        \"Hilarious from beginning to end.\",\n",
    "        \"Left me emotionally drained — in a good way.\",\n",
    "        \"Might be the worst thing I've ever seen.\",\n",
    "        \"This film restored my faith in cinema.\",\n",
    "        \"Cheap effects and poor editing.\",\n",
    "        \"A deeply moving experience.\",\n",
    "        \"Embarrassingly bad.\",\n",
    "        \"Couldn't stop smiling the whole time.\",\n",
    "        \"They clearly didn’t care about quality.\",\n",
    "        \"Highly recommended!\",\n",
    "        \"Too many plot holes to count.\",\n",
    "        \"Incredibly well written.\",\n",
    "        \"Poorly thought-out mess.\",\n",
    "        \"An emotional rollercoaster.\",\n",
    "        \"Visually noisy and confusing.\",\n",
    "        \"Soft, gentle, and powerful.\",\n",
    "        \"Why did I even watch this?\",\n",
    "        \"Unforgettable in the best sense.\",\n",
    "        \"Terribly disappointing.\",\n",
    "        \"It gave me chills.\",\n",
    "        \"Felt more like a parody.\",\n",
    "        \"Exactly what I needed today.\",\n",
    "        \"Not as clever as it thinks it is.\",\n",
    "        \"It spoke directly to my heart.\",\n",
    "        \"Wish I had skipped it.\",\n",
    "        \"A masterclass in subtlety.\",\n",
    "        \"The acting was unbearable.\",\n",
    "        \"Loved every second of it.\",\n",
    "        \"Just plain bad.\",\n",
    "        \"It’s rare to see something this sincere.\"\n",
    "    ],\n",
    "    \"labels\": [\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        1, 0, 1, 0, 1,\n",
    "        0\n",
    "    ]\n",
    "}\n",
    "\n",
    "def evaluate_model_on_independent_data(model, model_name, vectorizer=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the independent evaluation set.\n",
    "    If a vectorizer is provided, transform the sentences; otherwise, assume the model\n",
    "    can handle raw text (e.g., BERT pipelines).\n",
    "    Prints per-sentence predictions and a classification report.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluation for: {model_name} ---\")\n",
    "    texts = independent_eval_data[\"sentences\"]\n",
    "    true_labels = independent_eval_data[\"labels\"]\n",
    "\n",
    "    if vectorizer is not None:\n",
    "        features = vectorizer.transform(texts)\n",
    "    else:\n",
    "        features = texts  # model.predict should accept raw texts\n",
    "\n",
    "    predictions = model.predict(features)\n",
    "\n",
    "    print(\"\\nPredictions on independent sentences:\")\n",
    "    for sentence, pred, true in zip(texts, predictions, true_labels):\n",
    "        pred_label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        true_label = \"Positive\" if true == 1 else \"Negative\"\n",
    "        print(f\"Pred: {pred_label:<8} | True: {true_label:<8} | {sentence}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy on independent test set: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Logistic Regression (hold-out test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75      1098\n",
      "           1       0.73      0.76      0.74      1035\n",
      "\n",
      "    accuracy                           0.75      2133\n",
      "   macro avg       0.75      0.75      0.75      2133\n",
      "weighted avg       0.75      0.75      0.75      2133\n",
      "\n",
      "\n",
      "--- Evaluation for: TF-IDF + Logistic Regression ---\n",
      "\n",
      "Predictions on independent sentences:\n",
      "Pred: Negative | True: Negative | I can't believe I wasted two hours on this.\n",
      "Pred: Positive | True: Positive | Absolutely stunning visuals and sound.\n",
      "Pred: Negative | True: Negative | Nothing made sense from start to finish.\n",
      "Pred: Negative | True: Positive | I would gladly watch it again.\n",
      "Pred: Negative | True: Negative | The characters were flat and uninteresting.\n",
      "Pred: Positive | True: Positive | A truly heartwarming story.\n",
      "Pred: Negative | True: Negative | Regretted not walking out halfway.\n",
      "Pred: Negative | True: Positive | The pacing was perfect throughout.\n",
      "Pred: Negative | True: Negative | It felt like a school project.\n",
      "Pred: Positive | True: Positive | One of the best experiences I've had recently.\n",
      "Pred: Negative | True: Negative | I didn't expect much, and I was right.\n",
      "Pred: Positive | True: Positive | Everything came together beautifully.\n",
      "Pred: Positive | True: Negative | It lacked soul and depth.\n",
      "Pred: Positive | True: Positive | A breath of fresh air!\n",
      "Pred: Positive | True: Negative | I've seen better student films.\n",
      "Pred: Positive | True: Positive | Brilliantly acted and directed.\n",
      "Pred: Negative | True: Negative | It was just okay, nothing more.\n",
      "Pred: Positive | True: Positive | Truly inspiring and thoughtful.\n",
      "Pred: Negative | True: Negative | Worst decision to spend money on this.\n",
      "Pred: Positive | True: Positive | Delivers on every level.\n",
      "Pred: Negative | True: Negative | I nearly fell asleep watching it.\n",
      "Pred: Negative | True: Positive | Left me with tears of joy.\n",
      "Pred: Negative | True: Negative | Painfully dull and repetitive.\n",
      "Pred: Negative | True: Positive | I felt connected to every character.\n",
      "Pred: Negative | True: Negative | Should've trusted the negative reviews.\n",
      "Pred: Negative | True: Positive | Exceeded all expectations!\n",
      "Pred: Negative | True: Negative | The dialogue felt forced and fake.\n",
      "Pred: Positive | True: Positive | I'll remember this for years to come.\n",
      "Pred: Negative | True: Negative | Just another generic story.\n",
      "Pred: Positive | True: Positive | Packed with emotion and honesty.\n",
      "Pred: Negative | True: Negative | Everything was overacted.\n",
      "Pred: Positive | True: Positive | An instant favorite of mine.\n",
      "Pred: Positive | True: Negative | Confusing and messy throughout.\n",
      "Pred: Negative | True: Positive | The cinematography was breathtaking.\n",
      "Pred: Negative | True: Negative | Avoid at all costs.\n",
      "Pred: Negative | True: Positive | Finally, something original!\n",
      "Pred: Positive | True: Negative | I couldn’t finish it.\n",
      "Pred: Positive | True: Positive | Perfect from start to finish.\n",
      "Pred: Negative | True: Negative | So bad it’s almost good.\n",
      "Pred: Positive | True: Positive | Uplifting and magical.\n",
      "Pred: Negative | True: Negative | The twist was completely predictable.\n",
      "Pred: Negative | True: Positive | Left me speechless in the best way.\n",
      "Pred: Negative | True: Negative | Cliché after cliché.\n",
      "Pred: Positive | True: Positive | Creative and full of surprises.\n",
      "Pred: Negative | True: Negative | What a waste of potential.\n",
      "Pred: Positive | True: Positive | Full of charm and wit.\n",
      "Pred: Negative | True: Negative | Annoyingly loud soundtrack.\n",
      "Pred: Positive | True: Positive | Simply delightful!\n",
      "Pred: Negative | True: Negative | The plot went nowhere.\n",
      "Pred: Negative | True: Positive | A total joy to watch.\n",
      "Pred: Negative | True: Negative | I’ve never cringed so much.\n",
      "Pred: Positive | True: Positive | Soothing and beautiful tone.\n",
      "Pred: Negative | True: Negative | Disappointing in every way.\n",
      "Pred: Positive | True: Positive | Deserves an award.\n",
      "Pred: Positive | True: Negative | Awkward performances throughout.\n",
      "Pred: Positive | True: Positive | A rare gem these days.\n",
      "Pred: Negative | True: Negative | I expected more from this director.\n",
      "Pred: Negative | True: Positive | Left me smiling the whole time.\n",
      "Pred: Negative | True: Negative | Boring, dry, and uneventful.\n",
      "Pred: Positive | True: Positive | Smart, funny, and touching.\n",
      "Pred: Negative | True: Negative | Felt like a chore to sit through.\n",
      "Pred: Positive | True: Positive | Absolutely nailed it.\n",
      "Pred: Negative | True: Negative | Zero chemistry between the leads.\n",
      "Pred: Positive | True: Positive | Full of heart and meaning.\n",
      "Pred: Negative | True: Negative | Not even my dog enjoyed it.\n",
      "Pred: Negative | True: Positive | I didn’t want it to end.\n",
      "Pred: Negative | True: Negative | Cringe-worthy in all the wrong ways.\n",
      "Pred: Positive | True: Positive | Memorable and masterfully told.\n",
      "Pred: Positive | True: Positive | Felt rushed and incomplete.\n",
      "Pred: Positive | True: Negative | A celebration of storytelling.\n",
      "Pred: Negative | True: Positive | I'd rather do taxes than watch this again.\n",
      "Pred: Positive | True: Negative | Hilarious from beginning to end.\n",
      "Pred: Negative | True: Positive | Left me emotionally drained — in a good way.\n",
      "Pred: Negative | True: Negative | Might be the worst thing I've ever seen.\n",
      "Pred: Positive | True: Positive | This film restored my faith in cinema.\n",
      "Pred: Negative | True: Negative | Cheap effects and poor editing.\n",
      "Pred: Positive | True: Positive | A deeply moving experience.\n",
      "Pred: Negative | True: Negative | Embarrassingly bad.\n",
      "Pred: Negative | True: Positive | Couldn't stop smiling the whole time.\n",
      "Pred: Negative | True: Negative | They clearly didn’t care about quality.\n",
      "Pred: Positive | True: Positive | Highly recommended!\n",
      "Pred: Negative | True: Negative | Too many plot holes to count.\n",
      "Pred: Positive | True: Positive | Incredibly well written.\n",
      "Pred: Negative | True: Negative | Poorly thought-out mess.\n",
      "Pred: Positive | True: Positive | An emotional rollercoaster.\n",
      "Pred: Positive | True: Negative | Visually noisy and confusing.\n",
      "Pred: Positive | True: Positive | Soft, gentle, and powerful.\n",
      "Pred: Negative | True: Negative | Why did I even watch this?\n",
      "Pred: Positive | True: Positive | Unforgettable in the best sense.\n",
      "Pred: Negative | True: Negative | Terribly disappointing.\n",
      "Pred: Negative | True: Positive | It gave me chills.\n",
      "Pred: Negative | True: Negative | Felt more like a parody.\n",
      "Pred: Negative | True: Positive | Exactly what I needed today.\n",
      "Pred: Negative | True: Negative | Not as clever as it thinks it is.\n",
      "Pred: Positive | True: Positive | It spoke directly to my heart.\n",
      "Pred: Negative | True: Positive | Wish I had skipped it.\n",
      "Pred: Negative | True: Negative | A masterclass in subtlety.\n",
      "Pred: Negative | True: Positive | The acting was unbearable.\n",
      "Pred: Positive | True: Negative | Loved every second of it.\n",
      "Pred: Negative | True: Positive | Just plain bad.\n",
      "Pred: Positive | True: Negative | It’s rare to see something this sincere.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.80      0.73        50\n",
      "           1       0.76      0.63      0.69        51\n",
      "\n",
      "    accuracy                           0.71       101\n",
      "   macro avg       0.72      0.71      0.71       101\n",
      "weighted avg       0.72      0.71      0.71       101\n",
      "\n",
      "Accuracy on independent test set: 0.71\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Logistic Regression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Vectorize training and test texts\n",
    "tfidf = TfidfVectorizer(min_df=3, max_features=10000)\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression on TF-IDF vectors\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate on hold-out test set\n",
    "y_pred = clf_tfidf.predict(X_test_vec)\n",
    "print(\"TF-IDF + Logistic Regression (hold-out test)\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluation by sentences\n",
    "evaluate_model_on_independent_data(\n",
    "    model=clf_tfidf,\n",
    "    model_name=\"TF-IDF + Logistic Regression\",\n",
    "    vectorizer=tfidf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/stacey_xd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec + Logistic Regression (hold-out test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.52      0.56      1098\n",
      "           1       0.55      0.63      0.59      1035\n",
      "\n",
      "    accuracy                           0.57      2133\n",
      "   macro avg       0.58      0.58      0.57      2133\n",
      "weighted avg       0.58      0.57      0.57      2133\n",
      "\n",
      "\n",
      "--- Evaluation for: Word2Vec + Logistic Regression ---\n",
      "\n",
      "Predictions on independent sentences:\n",
      "Pred: Negative | True: Negative | I can't believe I wasted two hours on this.\n",
      "Pred: Positive | True: Positive | Absolutely stunning visuals and sound.\n",
      "Pred: Negative | True: Negative | Nothing made sense from start to finish.\n",
      "Pred: Negative | True: Positive | I would gladly watch it again.\n",
      "Pred: Positive | True: Negative | The characters were flat and uninteresting.\n",
      "Pred: Positive | True: Positive | A truly heartwarming story.\n",
      "Pred: Negative | True: Negative | Regretted not walking out halfway.\n",
      "Pred: Negative | True: Positive | The pacing was perfect throughout.\n",
      "Pred: Negative | True: Negative | It felt like a school project.\n",
      "Pred: Negative | True: Positive | One of the best experiences I've had recently.\n",
      "Pred: Negative | True: Negative | I didn't expect much, and I was right.\n",
      "Pred: Negative | True: Positive | Everything came together beautifully.\n",
      "Pred: Negative | True: Negative | It lacked soul and depth.\n",
      "Pred: Positive | True: Positive | A breath of fresh air!\n",
      "Pred: Negative | True: Negative | I've seen better student films.\n",
      "Pred: Positive | True: Positive | Brilliantly acted and directed.\n",
      "Pred: Negative | True: Negative | It was just okay, nothing more.\n",
      "Pred: Positive | True: Positive | Truly inspiring and thoughtful.\n",
      "Pred: Negative | True: Negative | Worst decision to spend money on this.\n",
      "Pred: Negative | True: Positive | Delivers on every level.\n",
      "Pred: Negative | True: Negative | I nearly fell asleep watching it.\n",
      "Pred: Positive | True: Positive | Left me with tears of joy.\n",
      "Pred: Positive | True: Negative | Painfully dull and repetitive.\n",
      "Pred: Negative | True: Positive | I felt connected to every character.\n",
      "Pred: Negative | True: Negative | Should've trusted the negative reviews.\n",
      "Pred: Negative | True: Positive | Exceeded all expectations!\n",
      "Pred: Positive | True: Negative | The dialogue felt forced and fake.\n",
      "Pred: Negative | True: Positive | I'll remember this for years to come.\n",
      "Pred: Negative | True: Negative | Just another generic story.\n",
      "Pred: Positive | True: Positive | Packed with emotion and honesty.\n",
      "Pred: Negative | True: Negative | Everything was overacted.\n",
      "Pred: Positive | True: Positive | An instant favorite of mine.\n",
      "Pred: Positive | True: Negative | Confusing and messy throughout.\n",
      "Pred: Negative | True: Positive | The cinematography was breathtaking.\n",
      "Pred: Negative | True: Negative | Avoid at all costs.\n",
      "Pred: Positive | True: Positive | Finally, something original!\n",
      "Pred: Negative | True: Negative | I couldn’t finish it.\n",
      "Pred: Negative | True: Positive | Perfect from start to finish.\n",
      "Pred: Negative | True: Negative | So bad it’s almost good.\n",
      "Pred: Positive | True: Positive | Uplifting and magical.\n",
      "Pred: Negative | True: Negative | The twist was completely predictable.\n",
      "Pred: Positive | True: Positive | Left me speechless in the best way.\n",
      "Pred: Negative | True: Negative | Cliché after cliché.\n",
      "Pred: Positive | True: Positive | Creative and full of surprises.\n",
      "Pred: Positive | True: Negative | What a waste of potential.\n",
      "Pred: Positive | True: Positive | Full of charm and wit.\n",
      "Pred: Negative | True: Negative | Annoyingly loud soundtrack.\n",
      "Pred: Negative | True: Positive | Simply delightful!\n",
      "Pred: Positive | True: Negative | The plot went nowhere.\n",
      "Pred: Negative | True: Positive | A total joy to watch.\n",
      "Pred: Negative | True: Negative | I’ve never cringed so much.\n",
      "Pred: Positive | True: Positive | Soothing and beautiful tone.\n",
      "Pred: Positive | True: Negative | Disappointing in every way.\n",
      "Pred: Positive | True: Positive | Deserves an award.\n",
      "Pred: Negative | True: Negative | Awkward performances throughout.\n",
      "Pred: Negative | True: Positive | A rare gem these days.\n",
      "Pred: Negative | True: Negative | I expected more from this director.\n",
      "Pred: Negative | True: Positive | Left me smiling the whole time.\n",
      "Pred: Positive | True: Negative | Boring, dry, and uneventful.\n",
      "Pred: Positive | True: Positive | Smart, funny, and touching.\n",
      "Pred: Negative | True: Negative | Felt like a chore to sit through.\n",
      "Pred: Negative | True: Positive | Absolutely nailed it.\n",
      "Pred: Positive | True: Negative | Zero chemistry between the leads.\n",
      "Pred: Positive | True: Positive | Full of heart and meaning.\n",
      "Pred: Negative | True: Negative | Not even my dog enjoyed it.\n",
      "Pred: Negative | True: Positive | I didn’t want it to end.\n",
      "Pred: Positive | True: Negative | Cringe-worthy in all the wrong ways.\n",
      "Pred: Positive | True: Positive | Memorable and masterfully told.\n",
      "Pred: Positive | True: Positive | Felt rushed and incomplete.\n",
      "Pred: Positive | True: Negative | A celebration of storytelling.\n",
      "Pred: Negative | True: Positive | I'd rather do taxes than watch this again.\n",
      "Pred: Negative | True: Negative | Hilarious from beginning to end.\n",
      "Pred: Negative | True: Positive | Left me emotionally drained — in a good way.\n",
      "Pred: Negative | True: Negative | Might be the worst thing I've ever seen.\n",
      "Pred: Negative | True: Positive | This film restored my faith in cinema.\n",
      "Pred: Positive | True: Negative | Cheap effects and poor editing.\n",
      "Pred: Positive | True: Positive | A deeply moving experience.\n",
      "Pred: Negative | True: Negative | Embarrassingly bad.\n",
      "Pred: Negative | True: Positive | Couldn't stop smiling the whole time.\n",
      "Pred: Negative | True: Negative | They clearly didn’t care about quality.\n",
      "Pred: Negative | True: Positive | Highly recommended!\n",
      "Pred: Negative | True: Negative | Too many plot holes to count.\n",
      "Pred: Negative | True: Positive | Incredibly well written.\n",
      "Pred: Negative | True: Negative | Poorly thought-out mess.\n",
      "Pred: Positive | True: Positive | An emotional rollercoaster.\n",
      "Pred: Positive | True: Negative | Visually noisy and confusing.\n",
      "Pred: Positive | True: Positive | Soft, gentle, and powerful.\n",
      "Pred: Negative | True: Negative | Why did I even watch this?\n",
      "Pred: Positive | True: Positive | Unforgettable in the best sense.\n",
      "Pred: Negative | True: Negative | Terribly disappointing.\n",
      "Pred: Negative | True: Positive | It gave me chills.\n",
      "Pred: Negative | True: Negative | Felt more like a parody.\n",
      "Pred: Negative | True: Positive | Exactly what I needed today.\n",
      "Pred: Negative | True: Negative | Not as clever as it thinks it is.\n",
      "Pred: Negative | True: Positive | It spoke directly to my heart.\n",
      "Pred: Negative | True: Positive | Wish I had skipped it.\n",
      "Pred: Positive | True: Negative | A masterclass in subtlety.\n",
      "Pred: Negative | True: Positive | The acting was unbearable.\n",
      "Pred: Negative | True: Negative | Loved every second of it.\n",
      "Pred: Negative | True: Positive | Just plain bad.\n",
      "Pred: Negative | True: Negative | It’s rare to see something this sincere.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.72      0.63        50\n",
      "           1       0.62      0.45      0.52        51\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.59      0.59      0.58       101\n",
      "weighted avg       0.59      0.58      0.58       101\n",
      "\n",
      "Accuracy on independent test set: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec + Logistic Regression\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize training corpus for Word2Vec training\n",
    "X_train_tok = [word_tokenize(text.lower()) for text in X_train]\n",
    "\n",
    "# Train Word2Vec model on the tokenized training texts\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sentences=X_train_tok,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Define a vectorizer class to compute average Word2Vec embeddings\n",
    "class Word2VecVectorizer:\n",
    "    def __init__(self, model, vector_size=100):\n",
    "        self.model = model\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def transform(self, texts):\n",
    "        tokenized = [word_tokenize(text.lower()) for text in texts]\n",
    "        return np.array([self.document_vector(tokens) for tokens in tokenized])\n",
    "\n",
    "    def document_vector(self, tokens):\n",
    "        words = [w for w in tokens if w in self.model.wv]\n",
    "        if words:\n",
    "            return np.mean([self.model.wv[w] for w in words], axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "# Initialize vectorizer and transform train/test sets\n",
    "w2v_vectorizer = Word2VecVectorizer(w2v_model)\n",
    "X_train_w2v = w2v_vectorizer.transform(X_train)\n",
    "X_test_w2v = w2v_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression on Word2Vec vectors\n",
    "clf_w2v = LogisticRegression(max_iter=1000)\n",
    "clf_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate on hold-out test set\n",
    "y_pred_w2v = clf_w2v.predict(X_test_w2v)\n",
    "print(\"\\nWord2Vec + Logistic Regression (hold-out test)\")\n",
    "print(classification_report(y_test, y_pred_w2v))\n",
    "\n",
    "# Evaluation by sentences\n",
    "evaluate_model_on_independent_data(\n",
    "    model=clf_w2v,\n",
    "    model_name=\"Word2Vec + Logistic Regression\",\n",
    "    vectorizer=w2v_vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec + XGBoost (hold-out test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56      1098\n",
      "           1       0.54      0.57      0.55      1035\n",
      "\n",
      "    accuracy                           0.56      2133\n",
      "   macro avg       0.56      0.56      0.56      2133\n",
      "weighted avg       0.56      0.56      0.56      2133\n",
      "\n",
      "\n",
      "--- Evaluation for: Word2Vec + XGBoost ---\n",
      "\n",
      "Predictions on independent sentences:\n",
      "Pred: Negative | True: Negative | I can't believe I wasted two hours on this.\n",
      "Pred: Positive | True: Positive | Absolutely stunning visuals and sound.\n",
      "Pred: Negative | True: Negative | Nothing made sense from start to finish.\n",
      "Pred: Negative | True: Positive | I would gladly watch it again.\n",
      "Pred: Negative | True: Negative | The characters were flat and uninteresting.\n",
      "Pred: Positive | True: Positive | A truly heartwarming story.\n",
      "Pred: Positive | True: Negative | Regretted not walking out halfway.\n",
      "Pred: Negative | True: Positive | The pacing was perfect throughout.\n",
      "Pred: Negative | True: Negative | It felt like a school project.\n",
      "Pred: Negative | True: Positive | One of the best experiences I've had recently.\n",
      "Pred: Negative | True: Negative | I didn't expect much, and I was right.\n",
      "Pred: Negative | True: Positive | Everything came together beautifully.\n",
      "Pred: Positive | True: Negative | It lacked soul and depth.\n",
      "Pred: Positive | True: Positive | A breath of fresh air!\n",
      "Pred: Negative | True: Negative | I've seen better student films.\n",
      "Pred: Positive | True: Positive | Brilliantly acted and directed.\n",
      "Pred: Positive | True: Negative | It was just okay, nothing more.\n",
      "Pred: Positive | True: Positive | Truly inspiring and thoughtful.\n",
      "Pred: Positive | True: Negative | Worst decision to spend money on this.\n",
      "Pred: Positive | True: Positive | Delivers on every level.\n",
      "Pred: Negative | True: Negative | I nearly fell asleep watching it.\n",
      "Pred: Positive | True: Positive | Left me with tears of joy.\n",
      "Pred: Positive | True: Negative | Painfully dull and repetitive.\n",
      "Pred: Negative | True: Positive | I felt connected to every character.\n",
      "Pred: Negative | True: Negative | Should've trusted the negative reviews.\n",
      "Pred: Positive | True: Positive | Exceeded all expectations!\n",
      "Pred: Positive | True: Negative | The dialogue felt forced and fake.\n",
      "Pred: Negative | True: Positive | I'll remember this for years to come.\n",
      "Pred: Negative | True: Negative | Just another generic story.\n",
      "Pred: Positive | True: Positive | Packed with emotion and honesty.\n",
      "Pred: Negative | True: Negative | Everything was overacted.\n",
      "Pred: Positive | True: Positive | An instant favorite of mine.\n",
      "Pred: Positive | True: Negative | Confusing and messy throughout.\n",
      "Pred: Negative | True: Positive | The cinematography was breathtaking.\n",
      "Pred: Negative | True: Negative | Avoid at all costs.\n",
      "Pred: Negative | True: Positive | Finally, something original!\n",
      "Pred: Negative | True: Negative | I couldn’t finish it.\n",
      "Pred: Positive | True: Positive | Perfect from start to finish.\n",
      "Pred: Negative | True: Negative | So bad it’s almost good.\n",
      "Pred: Positive | True: Positive | Uplifting and magical.\n",
      "Pred: Negative | True: Negative | The twist was completely predictable.\n",
      "Pred: Positive | True: Positive | Left me speechless in the best way.\n",
      "Pred: Negative | True: Negative | Cliché after cliché.\n",
      "Pred: Negative | True: Positive | Creative and full of surprises.\n",
      "Pred: Negative | True: Negative | What a waste of potential.\n",
      "Pred: Negative | True: Positive | Full of charm and wit.\n",
      "Pred: Negative | True: Negative | Annoyingly loud soundtrack.\n",
      "Pred: Positive | True: Positive | Simply delightful!\n",
      "Pred: Negative | True: Negative | The plot went nowhere.\n",
      "Pred: Negative | True: Positive | A total joy to watch.\n",
      "Pred: Negative | True: Negative | I’ve never cringed so much.\n",
      "Pred: Positive | True: Positive | Soothing and beautiful tone.\n",
      "Pred: Negative | True: Negative | Disappointing in every way.\n",
      "Pred: Negative | True: Positive | Deserves an award.\n",
      "Pred: Negative | True: Negative | Awkward performances throughout.\n",
      "Pred: Positive | True: Positive | A rare gem these days.\n",
      "Pred: Negative | True: Negative | I expected more from this director.\n",
      "Pred: Negative | True: Positive | Left me smiling the whole time.\n",
      "Pred: Negative | True: Negative | Boring, dry, and uneventful.\n",
      "Pred: Positive | True: Positive | Smart, funny, and touching.\n",
      "Pred: Negative | True: Negative | Felt like a chore to sit through.\n",
      "Pred: Negative | True: Positive | Absolutely nailed it.\n",
      "Pred: Positive | True: Negative | Zero chemistry between the leads.\n",
      "Pred: Negative | True: Positive | Full of heart and meaning.\n",
      "Pred: Negative | True: Negative | Not even my dog enjoyed it.\n",
      "Pred: Negative | True: Positive | I didn’t want it to end.\n",
      "Pred: Positive | True: Negative | Cringe-worthy in all the wrong ways.\n",
      "Pred: Positive | True: Positive | Memorable and masterfully told.\n",
      "Pred: Positive | True: Positive | Felt rushed and incomplete.\n",
      "Pred: Positive | True: Negative | A celebration of storytelling.\n",
      "Pred: Negative | True: Positive | I'd rather do taxes than watch this again.\n",
      "Pred: Negative | True: Negative | Hilarious from beginning to end.\n",
      "Pred: Positive | True: Positive | Left me emotionally drained — in a good way.\n",
      "Pred: Positive | True: Negative | Might be the worst thing I've ever seen.\n",
      "Pred: Negative | True: Positive | This film restored my faith in cinema.\n",
      "Pred: Negative | True: Negative | Cheap effects and poor editing.\n",
      "Pred: Positive | True: Positive | A deeply moving experience.\n",
      "Pred: Positive | True: Negative | Embarrassingly bad.\n",
      "Pred: Negative | True: Positive | Couldn't stop smiling the whole time.\n",
      "Pred: Negative | True: Negative | They clearly didn’t care about quality.\n",
      "Pred: Positive | True: Positive | Highly recommended!\n",
      "Pred: Negative | True: Negative | Too many plot holes to count.\n",
      "Pred: Negative | True: Positive | Incredibly well written.\n",
      "Pred: Positive | True: Negative | Poorly thought-out mess.\n",
      "Pred: Positive | True: Positive | An emotional rollercoaster.\n",
      "Pred: Negative | True: Negative | Visually noisy and confusing.\n",
      "Pred: Negative | True: Positive | Soft, gentle, and powerful.\n",
      "Pred: Negative | True: Negative | Why did I even watch this?\n",
      "Pred: Positive | True: Positive | Unforgettable in the best sense.\n",
      "Pred: Positive | True: Negative | Terribly disappointing.\n",
      "Pred: Positive | True: Positive | It gave me chills.\n",
      "Pred: Negative | True: Negative | Felt more like a parody.\n",
      "Pred: Negative | True: Positive | Exactly what I needed today.\n",
      "Pred: Negative | True: Negative | Not as clever as it thinks it is.\n",
      "Pred: Negative | True: Positive | It spoke directly to my heart.\n",
      "Pred: Negative | True: Positive | Wish I had skipped it.\n",
      "Pred: Positive | True: Negative | A masterclass in subtlety.\n",
      "Pred: Negative | True: Positive | The acting was unbearable.\n",
      "Pred: Positive | True: Negative | Loved every second of it.\n",
      "Pred: Negative | True: Positive | Just plain bad.\n",
      "Pred: Negative | True: Negative | It’s rare to see something this sincere.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.68      0.62        50\n",
      "           1       0.61      0.49      0.54        51\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.59      0.59      0.58       101\n",
      "weighted avg       0.59      0.58      0.58       101\n",
      "\n",
      "Accuracy on independent test set: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec + XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Transform train/test sets using existing Word2VecVectorizer\n",
    "X_train_w2v = w2v_vectorizer.transform(X_train)\n",
    "X_test_w2v = w2v_vectorizer.transform(X_test)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "clf_xgb = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "clf_xgb.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate on hold-out test set\n",
    "y_pred_xgb = clf_xgb.predict(X_test_w2v)\n",
    "print(\"\\nWord2Vec + XGBoost (hold-out test)\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Evaluation by sentences\n",
    "evaluate_model_on_independent_data(\n",
    "    model=clf_xgb,\n",
    "    model_name=\"Word2Vec + XGBoost\",\n",
    "    vectorizer=w2v_vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec + Random Forest (hold-out test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58      1098\n",
      "           1       0.56      0.59      0.57      1035\n",
      "\n",
      "    accuracy                           0.58      2133\n",
      "   macro avg       0.58      0.58      0.58      2133\n",
      "weighted avg       0.58      0.58      0.58      2133\n",
      "\n",
      "\n",
      "--- Evaluation for: Word2Vec + Random Forest ---\n",
      "\n",
      "Predictions on independent sentences:\n",
      "Pred: Negative | True: Negative | I can't believe I wasted two hours on this.\n",
      "Pred: Negative | True: Positive | Absolutely stunning visuals and sound.\n",
      "Pred: Negative | True: Negative | Nothing made sense from start to finish.\n",
      "Pred: Negative | True: Positive | I would gladly watch it again.\n",
      "Pred: Positive | True: Negative | The characters were flat and uninteresting.\n",
      "Pred: Positive | True: Positive | A truly heartwarming story.\n",
      "Pred: Negative | True: Negative | Regretted not walking out halfway.\n",
      "Pred: Negative | True: Positive | The pacing was perfect throughout.\n",
      "Pred: Negative | True: Negative | It felt like a school project.\n",
      "Pred: Negative | True: Positive | One of the best experiences I've had recently.\n",
      "Pred: Negative | True: Negative | I didn't expect much, and I was right.\n",
      "Pred: Negative | True: Positive | Everything came together beautifully.\n",
      "Pred: Negative | True: Negative | It lacked soul and depth.\n",
      "Pred: Positive | True: Positive | A breath of fresh air!\n",
      "Pred: Negative | True: Negative | I've seen better student films.\n",
      "Pred: Negative | True: Positive | Brilliantly acted and directed.\n",
      "Pred: Negative | True: Negative | It was just okay, nothing more.\n",
      "Pred: Negative | True: Positive | Truly inspiring and thoughtful.\n",
      "Pred: Positive | True: Negative | Worst decision to spend money on this.\n",
      "Pred: Negative | True: Positive | Delivers on every level.\n",
      "Pred: Negative | True: Negative | I nearly fell asleep watching it.\n",
      "Pred: Positive | True: Positive | Left me with tears of joy.\n",
      "Pred: Negative | True: Negative | Painfully dull and repetitive.\n",
      "Pred: Negative | True: Positive | I felt connected to every character.\n",
      "Pred: Negative | True: Negative | Should've trusted the negative reviews.\n",
      "Pred: Negative | True: Positive | Exceeded all expectations!\n",
      "Pred: Negative | True: Negative | The dialogue felt forced and fake.\n",
      "Pred: Negative | True: Positive | I'll remember this for years to come.\n",
      "Pred: Negative | True: Negative | Just another generic story.\n",
      "Pred: Positive | True: Positive | Packed with emotion and honesty.\n",
      "Pred: Negative | True: Negative | Everything was overacted.\n",
      "Pred: Negative | True: Positive | An instant favorite of mine.\n",
      "Pred: Positive | True: Negative | Confusing and messy throughout.\n",
      "Pred: Negative | True: Positive | The cinematography was breathtaking.\n",
      "Pred: Negative | True: Negative | Avoid at all costs.\n",
      "Pred: Negative | True: Positive | Finally, something original!\n",
      "Pred: Negative | True: Negative | I couldn’t finish it.\n",
      "Pred: Negative | True: Positive | Perfect from start to finish.\n",
      "Pred: Negative | True: Negative | So bad it’s almost good.\n",
      "Pred: Positive | True: Positive | Uplifting and magical.\n",
      "Pred: Negative | True: Negative | The twist was completely predictable.\n",
      "Pred: Negative | True: Positive | Left me speechless in the best way.\n",
      "Pred: Negative | True: Negative | Cliché after cliché.\n",
      "Pred: Negative | True: Positive | Creative and full of surprises.\n",
      "Pred: Negative | True: Negative | What a waste of potential.\n",
      "Pred: Negative | True: Positive | Full of charm and wit.\n",
      "Pred: Positive | True: Negative | Annoyingly loud soundtrack.\n",
      "Pred: Positive | True: Positive | Simply delightful!\n",
      "Pred: Negative | True: Negative | The plot went nowhere.\n",
      "Pred: Negative | True: Positive | A total joy to watch.\n",
      "Pred: Negative | True: Negative | I’ve never cringed so much.\n",
      "Pred: Positive | True: Positive | Soothing and beautiful tone.\n",
      "Pred: Negative | True: Negative | Disappointing in every way.\n",
      "Pred: Negative | True: Positive | Deserves an award.\n",
      "Pred: Positive | True: Negative | Awkward performances throughout.\n",
      "Pred: Positive | True: Positive | A rare gem these days.\n",
      "Pred: Negative | True: Negative | I expected more from this director.\n",
      "Pred: Negative | True: Positive | Left me smiling the whole time.\n",
      "Pred: Negative | True: Negative | Boring, dry, and uneventful.\n",
      "Pred: Positive | True: Positive | Smart, funny, and touching.\n",
      "Pred: Negative | True: Negative | Felt like a chore to sit through.\n",
      "Pred: Negative | True: Positive | Absolutely nailed it.\n",
      "Pred: Positive | True: Negative | Zero chemistry between the leads.\n",
      "Pred: Negative | True: Positive | Full of heart and meaning.\n",
      "Pred: Negative | True: Negative | Not even my dog enjoyed it.\n",
      "Pred: Positive | True: Positive | I didn’t want it to end.\n",
      "Pred: Negative | True: Negative | Cringe-worthy in all the wrong ways.\n",
      "Pred: Positive | True: Positive | Memorable and masterfully told.\n",
      "Pred: Positive | True: Positive | Felt rushed and incomplete.\n",
      "Pred: Positive | True: Negative | A celebration of storytelling.\n",
      "Pred: Negative | True: Positive | I'd rather do taxes than watch this again.\n",
      "Pred: Negative | True: Negative | Hilarious from beginning to end.\n",
      "Pred: Negative | True: Positive | Left me emotionally drained — in a good way.\n",
      "Pred: Positive | True: Negative | Might be the worst thing I've ever seen.\n",
      "Pred: Negative | True: Positive | This film restored my faith in cinema.\n",
      "Pred: Negative | True: Negative | Cheap effects and poor editing.\n",
      "Pred: Positive | True: Positive | A deeply moving experience.\n",
      "Pred: Negative | True: Negative | Embarrassingly bad.\n",
      "Pred: Negative | True: Positive | Couldn't stop smiling the whole time.\n",
      "Pred: Negative | True: Negative | They clearly didn’t care about quality.\n",
      "Pred: Positive | True: Positive | Highly recommended!\n",
      "Pred: Negative | True: Negative | Too many plot holes to count.\n",
      "Pred: Negative | True: Positive | Incredibly well written.\n",
      "Pred: Negative | True: Negative | Poorly thought-out mess.\n",
      "Pred: Negative | True: Positive | An emotional rollercoaster.\n",
      "Pred: Negative | True: Negative | Visually noisy and confusing.\n",
      "Pred: Positive | True: Positive | Soft, gentle, and powerful.\n",
      "Pred: Negative | True: Negative | Why did I even watch this?\n",
      "Pred: Negative | True: Positive | Unforgettable in the best sense.\n",
      "Pred: Negative | True: Negative | Terribly disappointing.\n",
      "Pred: Positive | True: Positive | It gave me chills.\n",
      "Pred: Negative | True: Negative | Felt more like a parody.\n",
      "Pred: Negative | True: Positive | Exactly what I needed today.\n",
      "Pred: Negative | True: Negative | Not as clever as it thinks it is.\n",
      "Pred: Negative | True: Positive | It spoke directly to my heart.\n",
      "Pred: Negative | True: Positive | Wish I had skipped it.\n",
      "Pred: Positive | True: Negative | A masterclass in subtlety.\n",
      "Pred: Negative | True: Positive | The acting was unbearable.\n",
      "Pred: Negative | True: Negative | Loved every second of it.\n",
      "Pred: Negative | True: Positive | Just plain bad.\n",
      "Pred: Negative | True: Negative | It’s rare to see something this sincere.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.82      0.65        50\n",
      "           1       0.64      0.31      0.42        51\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.59      0.57      0.54       101\n",
      "weighted avg       0.59      0.56      0.53       101\n",
      "\n",
      "Accuracy on independent test set: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec + Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Transform train/test sets using existing Word2VecVectorizer\n",
    "X_train_w2v = w2v_vectorizer.transform(X_train)\n",
    "X_test_w2v = w2v_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_rf.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate on hold-out test set\n",
    "y_pred_rf = clf_rf.predict(X_test_w2v)\n",
    "print(\"\\nWord2Vec + Random Forest (hold-out test)\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Evaluation by sentences\n",
    "evaluate_model_on_independent_data(\n",
    "    model=clf_rf,\n",
    "    model_name=\"Word2Vec + Random Forest\",\n",
    "    vectorizer=w2v_vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89       534\n",
      "           1       0.87      0.89      0.88       466\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.89      0.89      1000\n",
      "weighted avg       0.89      0.89      0.89      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Predict a subset for performance reasons\n",
    "X_test_sample = X_test.iloc[:1000]\n",
    "y_test_sample = y_test.iloc[:1000]\n",
    "\n",
    "preds = [1 if r['label'] == 'POSITIVE' else 0 for r in classifier(X_test_sample.tolist(), truncation=True)]\n",
    "print(\"DistilBERT\")\n",
    "print(classification_report(y_test_sample, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for: DistilBERT ---\n",
      "\n",
      "Predictions on independent sentences:\n",
      "Pred: Negative | True: Negative | I can't believe I wasted two hours on this.\n",
      "Pred: Positive | True: Positive | Absolutely stunning visuals and sound.\n",
      "Pred: Negative | True: Negative | Nothing made sense from start to finish.\n",
      "Pred: Positive | True: Positive | I would gladly watch it again.\n",
      "Pred: Negative | True: Negative | The characters were flat and uninteresting.\n",
      "Pred: Positive | True: Positive | A truly heartwarming story.\n",
      "Pred: Negative | True: Negative | Regretted not walking out halfway.\n",
      "Pred: Positive | True: Positive | The pacing was perfect throughout.\n",
      "Pred: Negative | True: Negative | It felt like a school project.\n",
      "Pred: Positive | True: Positive | One of the best experiences I've had recently.\n",
      "Pred: Positive | True: Negative | I didn't expect much, and I was right.\n",
      "Pred: Positive | True: Positive | Everything came together beautifully.\n",
      "Pred: Negative | True: Negative | It lacked soul and depth.\n",
      "Pred: Positive | True: Positive | A breath of fresh air!\n",
      "Pred: Negative | True: Negative | I've seen better student films.\n",
      "Pred: Positive | True: Positive | Brilliantly acted and directed.\n",
      "Pred: Positive | True: Negative | It was just okay, nothing more.\n",
      "Pred: Positive | True: Positive | Truly inspiring and thoughtful.\n",
      "Pred: Negative | True: Negative | Worst decision to spend money on this.\n",
      "Pred: Positive | True: Positive | Delivers on every level.\n",
      "Pred: Negative | True: Negative | I nearly fell asleep watching it.\n",
      "Pred: Positive | True: Positive | Left me with tears of joy.\n",
      "Pred: Negative | True: Negative | Painfully dull and repetitive.\n",
      "Pred: Positive | True: Positive | I felt connected to every character.\n",
      "Pred: Negative | True: Negative | Should've trusted the negative reviews.\n",
      "Pred: Positive | True: Positive | Exceeded all expectations!\n",
      "Pred: Negative | True: Negative | The dialogue felt forced and fake.\n",
      "Pred: Positive | True: Positive | I'll remember this for years to come.\n",
      "Pred: Negative | True: Negative | Just another generic story.\n",
      "Pred: Positive | True: Positive | Packed with emotion and honesty.\n",
      "Pred: Negative | True: Negative | Everything was overacted.\n",
      "Pred: Positive | True: Positive | An instant favorite of mine.\n",
      "Pred: Negative | True: Negative | Confusing and messy throughout.\n",
      "Pred: Positive | True: Positive | The cinematography was breathtaking.\n",
      "Pred: Negative | True: Negative | Avoid at all costs.\n",
      "Pred: Positive | True: Positive | Finally, something original!\n",
      "Pred: Negative | True: Negative | I couldn’t finish it.\n",
      "Pred: Positive | True: Positive | Perfect from start to finish.\n",
      "Pred: Positive | True: Negative | So bad it’s almost good.\n",
      "Pred: Positive | True: Positive | Uplifting and magical.\n",
      "Pred: Negative | True: Negative | The twist was completely predictable.\n",
      "Pred: Positive | True: Positive | Left me speechless in the best way.\n",
      "Pred: Negative | True: Negative | Cliché after cliché.\n",
      "Pred: Positive | True: Positive | Creative and full of surprises.\n",
      "Pred: Negative | True: Negative | What a waste of potential.\n",
      "Pred: Positive | True: Positive | Full of charm and wit.\n",
      "Pred: Negative | True: Negative | Annoyingly loud soundtrack.\n",
      "Pred: Positive | True: Positive | Simply delightful!\n",
      "Pred: Negative | True: Negative | The plot went nowhere.\n",
      "Pred: Positive | True: Positive | A total joy to watch.\n",
      "Pred: Positive | True: Negative | I’ve never cringed so much.\n",
      "Pred: Positive | True: Positive | Soothing and beautiful tone.\n",
      "Pred: Negative | True: Negative | Disappointing in every way.\n",
      "Pred: Positive | True: Positive | Deserves an award.\n",
      "Pred: Negative | True: Negative | Awkward performances throughout.\n",
      "Pred: Positive | True: Positive | A rare gem these days.\n",
      "Pred: Negative | True: Negative | I expected more from this director.\n",
      "Pred: Positive | True: Positive | Left me smiling the whole time.\n",
      "Pred: Negative | True: Negative | Boring, dry, and uneventful.\n",
      "Pred: Positive | True: Positive | Smart, funny, and touching.\n",
      "Pred: Negative | True: Negative | Felt like a chore to sit through.\n",
      "Pred: Positive | True: Positive | Absolutely nailed it.\n",
      "Pred: Negative | True: Negative | Zero chemistry between the leads.\n",
      "Pred: Positive | True: Positive | Full of heart and meaning.\n",
      "Pred: Negative | True: Negative | Not even my dog enjoyed it.\n",
      "Pred: Negative | True: Positive | I didn’t want it to end.\n",
      "Pred: Negative | True: Negative | Cringe-worthy in all the wrong ways.\n",
      "Pred: Positive | True: Positive | Memorable and masterfully told.\n",
      "Pred: Negative | True: Positive | Felt rushed and incomplete.\n",
      "Pred: Positive | True: Negative | A celebration of storytelling.\n",
      "Pred: Negative | True: Positive | I'd rather do taxes than watch this again.\n",
      "Pred: Positive | True: Negative | Hilarious from beginning to end.\n",
      "Pred: Positive | True: Positive | Left me emotionally drained — in a good way.\n",
      "Pred: Negative | True: Negative | Might be the worst thing I've ever seen.\n",
      "Pred: Positive | True: Positive | This film restored my faith in cinema.\n",
      "Pred: Negative | True: Negative | Cheap effects and poor editing.\n",
      "Pred: Positive | True: Positive | A deeply moving experience.\n",
      "Pred: Negative | True: Negative | Embarrassingly bad.\n",
      "Pred: Positive | True: Positive | Couldn't stop smiling the whole time.\n",
      "Pred: Negative | True: Negative | They clearly didn’t care about quality.\n",
      "Pred: Positive | True: Positive | Highly recommended!\n",
      "Pred: Negative | True: Negative | Too many plot holes to count.\n",
      "Pred: Positive | True: Positive | Incredibly well written.\n",
      "Pred: Negative | True: Negative | Poorly thought-out mess.\n",
      "Pred: Positive | True: Positive | An emotional rollercoaster.\n",
      "Pred: Negative | True: Negative | Visually noisy and confusing.\n",
      "Pred: Positive | True: Positive | Soft, gentle, and powerful.\n",
      "Pred: Negative | True: Negative | Why did I even watch this?\n",
      "Pred: Positive | True: Positive | Unforgettable in the best sense.\n",
      "Pred: Negative | True: Negative | Terribly disappointing.\n",
      "Pred: Positive | True: Positive | It gave me chills.\n",
      "Pred: Negative | True: Negative | Felt more like a parody.\n",
      "Pred: Positive | True: Positive | Exactly what I needed today.\n",
      "Pred: Negative | True: Negative | Not as clever as it thinks it is.\n",
      "Pred: Positive | True: Positive | It spoke directly to my heart.\n",
      "Pred: Negative | True: Positive | Wish I had skipped it.\n",
      "Pred: Positive | True: Negative | A masterclass in subtlety.\n",
      "Pred: Negative | True: Positive | The acting was unbearable.\n",
      "Pred: Positive | True: Negative | Loved every second of it.\n",
      "Pred: Negative | True: Positive | Just plain bad.\n",
      "Pred: Positive | True: Negative | It’s rare to see something this sincere.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85        50\n",
      "           1       0.83      0.88      0.86        51\n",
      "\n",
      "    accuracy                           0.85       101\n",
      "   macro avg       0.85      0.85      0.85       101\n",
      "weighted avg       0.85      0.85      0.85       101\n",
      "\n",
      "Accuracy on independent test set: 0.85\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT Evaluation on Independent Data\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize pre-trained DistilBERT sentiment-analysis pipeline\n",
    "distilbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Wrap the pipeline so it supports .predict(texts)\n",
    "class DistilBERTWrapper:\n",
    "    def __init__(self, hf_pipeline):\n",
    "        self.pipeline = hf_pipeline\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        results = self.pipeline(sentences, truncation=True)\n",
    "        return [1 if r[\"label\"] == \"POSITIVE\" else 0 for r in results]\n",
    "\n",
    "clf_distilbert = DistilBERTWrapper(distilbert_pipeline)\n",
    "\n",
    "# Evaluation by sentences\n",
    "evaluate_model_on_independent_data(\n",
    "    model=clf_distilbert,\n",
    "    model_name=\"DistilBERT\",\n",
    "    vectorizer=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bc8ac9b09a41259bfee6898ffa1991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8b52f3f6db4d92abcaec3d923d0c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1067/1067 [08:03<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report after epoch 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85      1051\n",
      "           1       0.83      0.91      0.87      1082\n",
      "\n",
      "    accuracy                           0.86      2133\n",
      "   macro avg       0.86      0.86      0.86      2133\n",
      "weighted avg       0.86      0.86      0.86      2133\n",
      "\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1067/1067 [07:54<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report after epoch 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      1051\n",
      "           1       0.87      0.89      0.88      1082\n",
      "\n",
      "    accuracy                           0.88      2133\n",
      "   macro avg       0.88      0.88      0.88      2133\n",
      "weighted avg       0.88      0.88      0.88      2133\n",
      "\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1067/1067 [07:54<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report after epoch 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      1051\n",
      "           1       0.89      0.88      0.88      1082\n",
      "\n",
      "    accuracy                           0.88      2133\n",
      "   macro avg       0.88      0.88      0.88      2133\n",
      "weighted avg       0.88      0.88      0.88      2133\n",
      "\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1067/1067 [07:53<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report after epoch 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88      1051\n",
      "           1       0.87      0.89      0.88      1082\n",
      "\n",
      "    accuracy                           0.88      2133\n",
      "   macro avg       0.88      0.88      0.88      2133\n",
      "weighted avg       0.88      0.88      0.88      2133\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert_sentiment/tokenizer_config.json',\n",
       " 'bert_sentiment/special_tokens_map.json',\n",
       " 'bert_sentiment/vocab.txt',\n",
       " 'bert_sentiment/added_tokens.json',\n",
       " 'bert_sentiment/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT Training and Evaluation\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    get_scheduler\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Combine and shuffle texts and labels\n",
    "texts = texts_pos + texts_neg  # recall texts_pos (label=1) and texts_neg (label=0)\n",
    "labels = [1] * len(texts_pos) + [0] * len(texts_neg)\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "\n",
    "# Create HuggingFace dataset and split into train/test\n",
    "dataset = Dataset.from_dict({\"text\": list(texts), \"label\": list(labels)})\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Remove the original text column\n",
    "train_dataset = train_dataset.remove_columns(\"text\")\n",
    "test_dataset = test_dataset.remove_columns(\"text\")\n",
    "\n",
    "# Prepare DataLoaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=False, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Set device (MPS or CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 4\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            labels_batch = batch[\"labels\"]\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, axis=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels_batch.numpy())\n",
    "\n",
    "    print(f\"\\nClassification report after epoch {epoch + 1}:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    model.train()\n",
    "\n",
    "# Save the best model from the final epoch\n",
    "model.save_pretrained(\"bert_sentiment\")\n",
    "tokenizer.save_pretrained(\"bert_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for: BERT ---\n",
      "Pred: Negative | True: Negative | I can't believe I wasted two hours on this.\n",
      "Pred: Positive | True: Positive | Absolutely stunning visuals and sound.\n",
      "Pred: Negative | True: Negative | Nothing made sense from start to finish.\n",
      "Pred: Positive | True: Positive | I would gladly watch it again.\n",
      "Pred: Negative | True: Negative | The characters were flat and uninteresting.\n",
      "Pred: Positive | True: Positive | A truly heartwarming story.\n",
      "Pred: Positive | True: Negative | Regretted not walking out halfway.\n",
      "Pred: Positive | True: Positive | The pacing was perfect throughout.\n",
      "Pred: Negative | True: Negative | It felt like a school project.\n",
      "Pred: Positive | True: Positive | One of the best experiences I've had recently.\n",
      "Pred: Positive | True: Negative | I didn't expect much, and I was right.\n",
      "Pred: Positive | True: Positive | Everything came together beautifully.\n",
      "Pred: Negative | True: Negative | It lacked soul and depth.\n",
      "Pred: Positive | True: Positive | A breath of fresh air!\n",
      "Pred: Negative | True: Negative | I've seen better student films.\n",
      "Pred: Positive | True: Positive | Brilliantly acted and directed.\n",
      "Pred: Negative | True: Negative | It was just okay, nothing more.\n",
      "Pred: Positive | True: Positive | Truly inspiring and thoughtful.\n",
      "Pred: Negative | True: Negative | Worst decision to spend money on this.\n",
      "Pred: Positive | True: Positive | Delivers on every level.\n",
      "Pred: Negative | True: Negative | I nearly fell asleep watching it.\n",
      "Pred: Positive | True: Positive | Left me with tears of joy.\n",
      "Pred: Negative | True: Negative | Painfully dull and repetitive.\n",
      "Pred: Positive | True: Positive | I felt connected to every character.\n",
      "Pred: Negative | True: Negative | Should've trusted the negative reviews.\n",
      "Pred: Positive | True: Positive | Exceeded all expectations!\n",
      "Pred: Negative | True: Negative | The dialogue felt forced and fake.\n",
      "Pred: Positive | True: Positive | I'll remember this for years to come.\n",
      "Pred: Negative | True: Negative | Just another generic story.\n",
      "Pred: Positive | True: Positive | Packed with emotion and honesty.\n",
      "Pred: Negative | True: Negative | Everything was overacted.\n",
      "Pred: Positive | True: Positive | An instant favorite of mine.\n",
      "Pred: Negative | True: Negative | Confusing and messy throughout.\n",
      "Pred: Positive | True: Positive | The cinematography was breathtaking.\n",
      "Pred: Negative | True: Negative | Avoid at all costs.\n",
      "Pred: Positive | True: Positive | Finally, something original!\n",
      "Pred: Negative | True: Negative | I couldn’t finish it.\n",
      "Pred: Positive | True: Positive | Perfect from start to finish.\n",
      "Pred: Negative | True: Negative | So bad it’s almost good.\n",
      "Pred: Positive | True: Positive | Uplifting and magical.\n",
      "Pred: Negative | True: Negative | The twist was completely predictable.\n",
      "Pred: Positive | True: Positive | Left me speechless in the best way.\n",
      "Pred: Negative | True: Negative | Cliché after cliché.\n",
      "Pred: Positive | True: Positive | Creative and full of surprises.\n",
      "Pred: Negative | True: Negative | What a waste of potential.\n",
      "Pred: Positive | True: Positive | Full of charm and wit.\n",
      "Pred: Negative | True: Negative | Annoyingly loud soundtrack.\n",
      "Pred: Positive | True: Positive | Simply delightful!\n",
      "Pred: Negative | True: Negative | The plot went nowhere.\n",
      "Pred: Positive | True: Positive | A total joy to watch.\n",
      "Pred: Negative | True: Negative | I’ve never cringed so much.\n",
      "Pred: Positive | True: Positive | Soothing and beautiful tone.\n",
      "Pred: Negative | True: Negative | Disappointing in every way.\n",
      "Pred: Positive | True: Positive | Deserves an award.\n",
      "Pred: Negative | True: Negative | Awkward performances throughout.\n",
      "Pred: Positive | True: Positive | A rare gem these days.\n",
      "Pred: Negative | True: Negative | I expected more from this director.\n",
      "Pred: Positive | True: Positive | Left me smiling the whole time.\n",
      "Pred: Negative | True: Negative | Boring, dry, and uneventful.\n",
      "Pred: Positive | True: Positive | Smart, funny, and touching.\n",
      "Pred: Negative | True: Negative | Felt like a chore to sit through.\n",
      "Pred: Positive | True: Positive | Absolutely nailed it.\n",
      "Pred: Negative | True: Negative | Zero chemistry between the leads.\n",
      "Pred: Positive | True: Positive | Full of heart and meaning.\n",
      "Pred: Negative | True: Negative | Not even my dog enjoyed it.\n",
      "Pred: Positive | True: Positive | I didn’t want it to end.\n",
      "Pred: Negative | True: Negative | Cringe-worthy in all the wrong ways.\n",
      "Pred: Positive | True: Positive | Memorable and masterfully told.\n",
      "Pred: Negative | True: Positive | Felt rushed and incomplete.\n",
      "Pred: Positive | True: Negative | A celebration of storytelling.\n",
      "Pred: Negative | True: Positive | I'd rather do taxes than watch this again.\n",
      "Pred: Positive | True: Negative | Hilarious from beginning to end.\n",
      "Pred: Negative | True: Positive | Left me emotionally drained — in a good way.\n",
      "Pred: Negative | True: Negative | Might be the worst thing I've ever seen.\n",
      "Pred: Positive | True: Positive | This film restored my faith in cinema.\n",
      "Pred: Negative | True: Negative | Cheap effects and poor editing.\n",
      "Pred: Positive | True: Positive | A deeply moving experience.\n",
      "Pred: Negative | True: Negative | Embarrassingly bad.\n",
      "Pred: Positive | True: Positive | Couldn't stop smiling the whole time.\n",
      "Pred: Negative | True: Negative | They clearly didn’t care about quality.\n",
      "Pred: Positive | True: Positive | Highly recommended!\n",
      "Pred: Negative | True: Negative | Too many plot holes to count.\n",
      "Pred: Positive | True: Positive | Incredibly well written.\n",
      "Pred: Negative | True: Negative | Poorly thought-out mess.\n",
      "Pred: Positive | True: Positive | An emotional rollercoaster.\n",
      "Pred: Negative | True: Negative | Visually noisy and confusing.\n",
      "Pred: Positive | True: Positive | Soft, gentle, and powerful.\n",
      "Pred: Negative | True: Negative | Why did I even watch this?\n",
      "Pred: Positive | True: Positive | Unforgettable in the best sense.\n",
      "Pred: Negative | True: Negative | Terribly disappointing.\n",
      "Pred: Positive | True: Positive | It gave me chills.\n",
      "Pred: Negative | True: Negative | Felt more like a parody.\n",
      "Pred: Positive | True: Positive | Exactly what I needed today.\n",
      "Pred: Negative | True: Negative | Not as clever as it thinks it is.\n",
      "Pred: Positive | True: Positive | It spoke directly to my heart.\n",
      "Pred: Negative | True: Positive | Wish I had skipped it.\n",
      "Pred: Positive | True: Negative | A masterclass in subtlety.\n",
      "Pred: Negative | True: Positive | The acting was unbearable.\n",
      "Pred: Positive | True: Negative | Loved every second of it.\n",
      "Pred: Negative | True: Positive | Just plain bad.\n",
      "Pred: Positive | True: Negative | It’s rare to see something this sincere.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87        50\n",
      "           1       0.87      0.88      0.87        51\n",
      "\n",
      "    accuracy                           0.87       101\n",
      "   macro avg       0.87      0.87      0.87       101\n",
      "weighted avg       0.87      0.87      0.87       101\n",
      "\n",
      "Accuracy on independent test set: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Load Saved BERT Model and Evaluate on Independent Sentences\n",
    "\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Path to the locally saved BERT model folder\n",
    "model_path = \"./bert_sentiment\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set device and switch to evaluation mode\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize independent sentences in a batch\n",
    "texts = independent_eval_data[\"sentences\"]\n",
    "true_labels = independent_eval_data[\"labels\"]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, axis=1).cpu().numpy()\n",
    "\n",
    "# Print per-sentence prediction vs true label\n",
    "print(\"\\n--- Evaluation for: BERT ---\")\n",
    "for sent, pred, true in zip(texts, predictions, true_labels):\n",
    "    pred_label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    true_label = \"Positive\" if true == 1 else \"Negative\"\n",
    "    print(f\"Pred: {pred_label:<8} | True: {true_label:<8} | {sent}\")\n",
    "\n",
    "# Print classification report and accuracy\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(f\"Accuracy on independent test set: {accuracy_score(true_labels, predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model quality metrics on the test dataset:\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Model                          |   Precision |   Recall |   F1-score |   Accuracy |\n",
      "+================================+=============+==========+============+============+\n",
      "| TF-IDF + Logistic Regression   |        0.75 |     0.75 |       0.75 |       0.75 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Word2Vec + Logistic Regression |        0.6  |     0.57 |       0.59 |       0.57 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Word2Vec + XGBoost             |        0.56 |     0.56 |       0.56 |       0.56 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Word2Vec + Random Forest       |        0.58 |     0.58 |       0.58 |       0.58 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| DistilBERT                     |        0.89 |     0.89 |       0.89 |       0.89 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| BERT (epoch 4)                 |        0.88 |     0.88 |       0.88 |       0.88 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data_holdout = [\n",
    "    [\"TF-IDF + Logistic Regression\",       0.75, 0.75, 0.75, 0.75],\n",
    "    [\"Word2Vec + Logistic Regression\",     0.60, 0.57, 0.59, 0.57],\n",
    "    [\"Word2Vec + XGBoost\",                 0.56, 0.56, 0.56, 0.56],\n",
    "    [\"Word2Vec + Random Forest\",           0.58, 0.58, 0.58, 0.58],\n",
    "    [\"DistilBERT\",                         0.89, 0.89, 0.89, 0.89],\n",
    "    [\"BERT (epoch 4)\",                     0.88, 0.88, 0.88, 0.88],\n",
    "]\n",
    "\n",
    "headers = [\"Model\", \"Precision\", \"Recall\", \"F1-score\", \"Accuracy\"]\n",
    "print(\"Model quality metrics on the test dataset:\")\n",
    "print(tabulate(data_holdout, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model quality metrics on an independent dataset:\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Model                          |   Precision |   Recall |   F1-score |   Accuracy |\n",
      "+================================+=============+==========+============+============+\n",
      "| TF-IDF + Logistic Regression   |        0.72 |     0.71 |       0.71 |       0.71 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Word2Vec + Logistic Regression |        0.59 |     0.58 |       0.58 |       0.58 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Word2Vec + XGBoost             |        0.59 |     0.58 |       0.58 |       0.58 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| Word2Vec + Random Forest       |        0.59 |     0.56 |       0.53 |       0.56 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| DistilBERT                     |        0.85 |     0.85 |       0.85 |       0.85 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n",
      "| BERT                           |        0.87 |     0.87 |       0.87 |       0.87 |\n",
      "+--------------------------------+-------------+----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data_holdout = [\n",
    "    [\"TF-IDF + Logistic Regression\",   0.72, 0.71, 0.71, 0.71],\n",
    "    [\"Word2Vec + Logistic Regression\", 0.59, 0.58, 0.58, 0.58],\n",
    "    [\"Word2Vec + XGBoost\",             0.59, 0.58, 0.58, 0.58],\n",
    "    [\"Word2Vec + Random Forest\",       0.59, 0.56, 0.53, 0.56],\n",
    "    [\"DistilBERT\",                     0.85, 0.85, 0.85, 0.85],\n",
    "    [\"BERT\",                           0.87, 0.87, 0.87, 0.87],\n",
    "]\n",
    "\n",
    "headers = [\"Model\", \"Precision\", \"Recall\", \"F1-score\", \"Accuracy\"]\n",
    "print(\"Model quality metrics on an independent dataset:\")\n",
    "print(tabulate(data_holdout, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis\n",
    "\n",
    "**Top Performers: BERT & DistilBERT (Accuracy: ~0.85-0.89)**  \n",
    "These Transformer-based models demonstrated superior performance by a large margin. Their key advantage is **contextual understanding**, allowing them to interpret a word's meaning based on the entire sentence. This leads to a more nuanced and accurate classification. Additionally, the BERT model was trained over four epochs, with the final epoch yielding the best results.\n",
    "\n",
    "**Strong Baseline: TF-IDF + Logistic Regression (Accuracy: ~0.71-0.75)**  \n",
    "This classical approach performed reasonably well. Its strength lies in identifying **important keywords** that are highly indicative of a specific class. While it ignores context and grammar, it proved to be an effective baseline for this task.\n",
    "\n",
    "**Underperformers: Word2Vec-based Models (Accuracy: ~0.56-0.59)**  \n",
    "All models using Word2Vec embeddings showed poor results, barely surpassing a random guess. The primary reason is the **loss of information** when averaging non-contextual word vectors to create a single document representation. This method dilutes the impact of key predictive words, providing a weak signal to the classifier.\n",
    "\n",
    "**Conclusion on Datasets**  \n",
    "The slight performance decrease on the **independent dataset** compared to the test set is expected. It provides a more realistic measure of each model's ability to generalize to new, unseen data. The overall ranking and performance gap between model types remained consistent, confirming the superiority of the Transformer architecture for this problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
